services:
  # Backend API Server
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=development
      - PORT=3001
      - PREFER_LOCAL_AI=false
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://backlog_user:${POSTGRES_PASSWORD}@postgres:5432/backlog_builder
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GITHUB_OWNER=${GITHUB_OWNER}
    volumes:
      - ./backend:/app
      - /app/node_modules
      - backend-logs:/app/logs
    depends_on:
      - redis
      - postgres
    networks:
      - backlog-network
    user: "node"

  # Frontend Development Server
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8085:8080"
    volumes:
      - ./frontend:/usr/src/app
      - /usr/src/app/node_modules
    environment:
      - NODE_ENV=development
      - VUE_APP_API_URL=http://backend:3001
    stdin_open: true
    tty: true
    depends_on:
      - backend
    networks:
      - backlog-network

  # PostgreSQL Database
  postgres:
    image: postgres:14-alpine
    environment:
      - POSTGRES_USER=backlog_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=backlog_builder
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backlog-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U backlog_user -d backlog_builder"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
      - backlog-network

  # Whisper Speech-to-Text Service
  whisper-stt:
    build:
      context: ./ai-services/speech-to-text
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PORT=8000
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./ai-services/speech-to-text:/app
      - whisper-models:/root/.cache/whisper
    networks:
      - backlog-network
    depends_on:
      - redis
    # Uncomment the following if you want to build models during container startup
    # command: >
    #   sh -c "python -c 'import whisper; whisper.load_model(\"base\")' &&
    #   uvicorn whisper-local:app --host 0.0.0.0 --port 8000"

  # Hugging Face Local LLM Service
  huggingface-llm:
    build:
      context: ./ai-services/llm-processing
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      - PORT=8002
      - REDIS_URL=redis://redis:6379
      - DEVICE=cuda  # or cpu if no GPU available
    volumes:
      - ./ai-services/llm-processing:/app
      - huggingface-models:/root/.cache/huggingface/hub
    networks:
      - backlog-network
    depends_on:
      - redis
    # Uncomment and modify the following if you want to pre-download models
    # command: >
    #   sh -c "python -c 'from transformers import AutoModelForSeq2SeqLM; AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")' &&
    #   uvicorn huggingface-local:app --host 0.0.0.0 --port 8002"

networks:
  backlog-network:
    driver: bridge

volumes:
  postgres_data:
  uploads:
  backend-logs:
  whisper-models:
  huggingface-models:
